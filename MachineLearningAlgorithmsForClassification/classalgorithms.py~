from __future__ import division  # floating point division
import numpy as np
import utilities as utils

class Classifier:
    """
    Generic classifier interface; returns random classification
    Assumes y in {0,1}, rather than {-1, 1}
    """
    
    def __init__( self, parameters={} ):
        """ Params can contain any useful parameters for the algorithm """
        self.params = {}
        
    def reset(self, parameters):
        """ Reset learner """
        self.resetparams(parameters)

    def resetparams(self, parameters):
        """ Can pass parameters to reset with new parameters """
        try:
            utils.update_dictionary_items(self.params,parameters)
        except AttributeError:
            # Variable self.params does not exist, so not updated
            # Create an empty set of params for future reference
            self.params = {}
            
    def getparams(self):
        return self.params
    
    def learn(self, Xtrain, ytrain):
        """ Learns using the traindata """
        
    def predict(self, Xtest):
        probs = np.random.rand(Xtest.shape[0])
        ytest = utils.threshold_probs(probs)
        return ytest

class LinearRegressionClass(Classifier):
    """
    Linear Regression with ridge regularization
    Simply solves (X.T X/t + lambda eye)^{-1} X.T y/t
    """
    def __init__( self, parameters={} ):
        self.params = {'regwgt': 0.01}
        self.reset(parameters)

    def reset(self, parameters):
        self.resetparams(parameters)
        self.weights = None

    def learn(self, Xtrain, ytrain):
        """ Learns using the traindata """
        # Ensure ytrain is {-1,1}
        yt = np.copy(ytrain)
        yt[yt == 0] = -1
        
        # Dividing by numsamples before adding ridge regularization
        # for additional stability; this also makes the
        # regularization parameter not dependent on numsamples
        # if want regularization disappear with more samples, must pass
        # such a regularization parameter lambda/t
        numsamples = Xtrain.shape[0]
        self.weights = np.dot(np.dot(np.linalg.pinv(np.add(np.dot(Xtrain.T,Xtrain)/numsamples,self.params['regwgt']*np.identity(Xtrain.shape[1]))), Xtrain.T),yt)/numsamples
        
    def predict(self, Xtest):
        ytest = np.dot(Xtest, self.weights)
        ytest[ytest > 0] = 1     
        ytest[ytest < 0] = 0    
        return ytest
        
class NaiveBayes(Classifier):
    """ Gaussian naive Bayes;  """
    
    def __init__( self, parameters={} ):
        """ Params can contain any useful parameters for the algorithm """
        # Assumes that a bias unit has been added to feature vector as the last feature
        # If usecolumnones is False, it ignores this last feature
        self.params = {'usecolumnones': False}
        self.reset(parameters)           
    def reset(self, parameters):
        self.resetparams(parameters)
        # TODO: set up required variables for learning
        
    # TODO: implement learn and predict functions 
    def learn(self, Xtrain, ytrain):   	
        if self.params['usecolumnones']== False:
		    Xtrain = Xtrain[:, :-1] 	#Removing last column of ones		
        TargetData = np.insert(Xtrain, Xtrain.shape[1], ytrain, axis=1)	#adding ytrain to the last column	
        Xtrain0 = TargetData[TargetData[:, (len(TargetData[0]) - 1)] == 0] #creating training set for Y=0
        Xtrain1 = TargetData[TargetData[:, (len(TargetData[0]) - 1)] == 1] #creating training set for Y=1
        self.XMean0 = Xtrain0.mean(axis=0)#calculating Mean Standard  deviation for both training sets with Y=0 and Y=1
        self.XStd0 = Xtrain0.std(axis=0)
        self.XMean1 = Xtrain1.mean(axis=0)
        self.XStd1 = Xtrain1.std(axis=0)

    def predict(self, Xtest):
        if self.params['usecolumnones']== False:
		    Xtest = Xtest[:, :-1]#Removing the last column of Ones from Test dataset
        ytest = []
        for test in Xtest:
            p0 = 1#initialinzing probability for y=0 and y=1 to 1
            p1 = 1			
            for i in range(test.shape[0]):#Calculating probabilites for the testset for Y=0 and Y=1
                p0 = p0 * (utils.calculateprob(test[i],self.XMean0[i],self.XStd0[i]))
                p1 = p1 *(utils.calculateprob(test[i],self.XMean1[i],self.XStd1[i]))

            if p1 >= p0:#Classifier criteria for Y=0 and Y=1 for predictions.
                ytest.append(1)
            else:
                ytest.append(0)

        return ytest
            			
class LogitReg(Classifier):

    def __init__( self, parameters={} ):
        # Default: no regularization
        self.params = {'regwgt': 0.0, 'regularizer': 'None'}
        self.reset(parameters)

    def reset(self, parameters):
        self.resetparams(parameters)
        self.weights = None
        if self.params['regularizer'] is 'l1':
            self.regularizer = (utils.l1, utils.dl1)
        elif self.params['regularizer'] is 'l2':
            self.regularizer = (utils.l2, utils.dl2)
        elif self.params['regularizer'] is 'l3':
            self.regularizer = (utils.l3, utils.l3)		
        else:
            self.regularizer = (lambda w: 0, lambda w: np.zeros(w.shape,))
     
    # TODO: implement learn and predict functions                  
    def learn(self, Xtrain, ytrain):
        """ Learns using the traindata """
        self.weights=np.dot(np.dot(np.linalg.pinv(np.dot(Xtrain.T,Xtrain)),Xtrain.T),ytrain)
        p=utils.sigmoid(np.dot(Xtrain,self.weights))
        Err=np.linalg.norm(np.subtract(ytrain,p))
        regwgt = self.params['regwgt']
        while(True):    		
            P=np.diag(p)
            self.weights=self.weights+np.dot(np.linalg.pinv(np.dot(np.dot(np.dot(Xtrain.T,P),(np.identity(P.shape[0])-p)),Xtrain)),np.dot(Xtrain.T,np.subtract(ytrain,p))+self.regularizer[1](self.weights)*regwgt)		
            p=utils.sigmoid(np.dot(Xtrain,self.weights))
            Error=np.linalg.norm(np.subtract(ytrain,p))
            if(abs(Err-Error)<0.01):
                break
            else:
                Err=Error 			
				
    def predict(self, Xtest):
        ytest = utils.sigmoid(np.dot(Xtest, self.weights))
        ytest[ytest >=0.5] = 1     
        ytest[ytest < 0.5] = 0    
        return ytest           


class NeuralNet(Classifier):

    def __init__(self, parameters={}):	
        self.params = {'nh': 4,
                        'transfer': 'sigmoid',
                        'stepsize': 0.01,
                        'epochs': 10}
                						
        self.reset(parameters)        

    def reset(self, parameters):
        self.resetparams(parameters)
        if self.params['transfer'] is 'sigmoid':
            self.transfer = utils.sigmoid
            self.dtransfer = utils.dsigmoid
        else:
            # For now, only allowing sigmoid transfer
            raise Exception('NeuralNet -> can only handle sigmoid transfer, must set option transfer to string sigmoid')      
        self.wi = None
        self.wo = None
        
    # TODO: implement learn and predict functions
    
    def sigmoid(self,xvec):

        return 1.0 / (1.0 + np.exp(np.negative(xvec)))

    def learn(self, Xtrain, ytrain):

        alpha=self.params['stepsize']
        self.wi=np.random.random(1,size=(Xtrain.shape[1], self.params['nh']))
        self.wo=np.random.randint(-1*self.params['nh'],self.params['nh'], size=(self.params['nh']))
        for iteration in range(self.params['epochs']):
            state = np.random.get_state()
            np.random.shuffle(Xtrain)
            np.random.set_state(state)
            np.random.shuffle(ytrain)
            for sample in range(Xtrain.shape[0]):
                z2=np.dot(Xtrain[sample], self.wi)
                h = utils.sigmoid(z2)
                z1=np.dot(h, self.wo)
                yt = self.sigmoid(z1)
                dr1 = yt - ytrain[sample]
                dw1 = dr1.T * h
                dr2=np.array([(self.wo * dr1)[i] * h[i] * (1 - h[i]) for i in range(len(self.wo))])
                dw2 = np.array([Xtrain[sample] * i for i in dr2]).T
                self.wi = self.wi - alpha * dw2
                self.wo = self.wo - alpha * dw1

    def predict(self, Xtest):
        z1 = np.dot(Xtest,self.wi)
        a1 = self.sigmoid(z1)
        z2 = np.dot(a1,self.wo)
        ytest = self.sigmoid(z2)
        ytest[ytest>=0.5]=1
        ytest[ytest<0.5]=0
        return ytest

class LogitRegAlternative(Classifier):

    def __init__( self, parameters={} ):
        self.reset(parameters)

    def reset(self, parameters):
        self.resetparams(parameters)
        self.weights = None
        
    # TODO: implement learn and predict functions                  
    def learn(self, Xtrain, ytrain):
        self.weights = np.dot(np.dot(np.linalg.inv(np.dot(Xtrain.T,Xtrain)),Xtrain.T),ytrain)
        self.steps=0.05		
        xvec = np.dot(Xtrain,self.weights)
        p =(1+xvec/np.sqrt(np.square(xvec)+1))/2 
        Err=np.linalg.norm(np.subtract(ytrain,p))			
        for j in range(500):  
            for i in range(Xtrain.shape[0]):
                xvec = np.dot(Xtrain[i], self.weights)
                gradient = np.dot(Xtrain[i].T,np.divide((2*ytrain[i]-1)*np.sqrt(np.square(xvec)+1)-xvec,np.square(xvec)+1))
                self.weights = self.weights +self.steps*gradient   				
             		
        		

    def predict(self, Xtest):
        xvec = np.dot(Xtest,self.weights)
        ytest = (1+xvec/np.sqrt(np.square(xvec)+1))/2
        ytest[ytest >= 0.5] = 1     
        ytest[ytest < 0.5] = 0    
        return ytest           

class RadialBasisNetwork(Classifier):

    def __init__( self, parameters={} ):
        # Default: no regularization
        self.params = {'regwgt': 0.0, 'regularizer': 'None'}
        self.reset(parameters)

    def reset(self, parameters):
        self.resetparams(parameters)
        self.weights = None
        if self.params['regularizer'] is 'l1':
            self.regularizer = (utils.l1, utils.dl1)
        elif self.params['regularizer'] is 'l2':
            self.regularizer = (utils.l2, utils.dl2)
        elif self.params['regularizer'] is 'l3':
            self.regularizer = (utils.l3, utils.l3)		
        else:
            self.regularizer = (lambda w: 0, lambda w: np.zeros(w.shape,))
            
    def _basisfunc(self, c, d):
         assert len(d) == self.indim
         return exp(-self.beta * norm(c-d)**2)
     
    def _calcAct(self, Xtrain):
        # calculate activations of RBFs
        G = zeros((Xtrain.shape[0], self.numCenters), float)
        for ci, c in enumerate(self.centers):
            for xi, x in enumerate(Xtrain):
                G[xi,ci] = self._basisfunc(c, x)
        return G
    # TODO: implement learn and predict functions                  
    def learn(self, Xtrain, ytrain):
        """ Learns using the traindata """
        rnd_idx = random.permutation(Xtrain.shape[0])[:self.numCenters]
        self.centers = [Xtrain[i,:] for i in rnd_idx]
         
        print "center", self.centers
        # calculate activations of RBFs
        Xtrain1 = self._calcAct(Xtrain)
        self.weights=np.dot(np.dot(np.linalg.pinv(np.dot(Xtrain.T,Xtrain)),Xtrain.T),ytrain)
        p=utils.sigmoid(np.dot(Xtrain,self.weights))
        Err=np.linalg.norm(np.subtract(ytrain,p))
        regwgt = self.params['regwgt']
        while(True):    		
            P=np.diag(p)
            self.weights=self.weights+np.dot(np.linalg.pinv(np.dot(np.dot(np.dot(Xtrain.T,P),(np.identity(P.shape[0])-p)),Xtrain)),np.dot(Xtrain.T,np.subtract(ytrain,p))+self.regularizer[1](self.weights)*regwgt)		
            p=utils.sigmoid(np.dot(Xtrain,self.weights))
            Error=np.linalg.norm(np.subtract(ytrain,p))
            if(abs(Err-Error)<0.01):
                break
            else:
                Err=Error 			
				
    def predict(self, Xtest):
        Xtest1= self._calcAct(Xtest)
        ytest = utils.sigmoid(np.dot(Xtest, self.weights))
        ytest[ytest >=0.5] = 1     
        ytest[ytest < 0.5] = 0    
        return ytest           


